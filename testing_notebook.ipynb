{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Individual\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vals: [ 0.6158239   0.84011077  0.04183522 -0.75900673  0.6951304  -0.97691133\n",
      "  0.77485725 -0.82851889 -0.95353989 -0.32360095]\n",
      "sigmas: [1.66666667 1.66666667 1.66666667 1.66666667 1.66666667 1.66666667\n",
      " 1.66666667 1.66666667 1.66666667 1.66666667]\n",
      "alphas: [3.42696966 0.64923298 0.8238042  4.85117295 5.13772174 1.056232\n",
      " 2.49537404 1.72231221 0.72279854 3.34894294 2.05771064 2.51616366\n",
      " 4.03218562 6.13131849 3.08254803 1.45737407 4.17943376 2.2948175\n",
      " 5.84752641 2.37427922 2.13293125 1.0710372  1.38996459 4.77501387\n",
      " 0.15083405 6.253388   5.75547902 3.11252595 0.50621058 0.85461134\n",
      " 4.15273275 3.99151359 0.40102153 4.00512595 2.10093107 3.45370772\n",
      " 3.50101416 5.44013019 5.92023285 1.66949865 0.30284108 4.99669553\n",
      " 4.7989411  2.09916004 5.4047313 ]\n"
     ]
    }
   ],
   "source": [
    "from classes.Individual import Individual\n",
    "i = Individual(10,5)\n",
    "print(f\"vals: {i.values}\")\n",
    "print(f\"sigmas: {i.sigmas}\")\n",
    "print(f\"alphas: {i.alphas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Individual: copy\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6158239 ,  0.84011077,  0.04183522, -0.75900673,  0.6951304 ,\n",
       "       -0.97691133,  0.77485725, -0.82851889, -0.95353989, -0.32360095])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_copy = i.copy_individual()\n",
    "i_copy.values = 54\n",
    "i.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Population\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fun(values):\n",
    "    return sum(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population shape: (n_individuals: 4, n_elements:5)\n",
      "\n",
      "all fitnesses: [ 1.62734578 -1.33564178  0.13268557  1.70415424]\n",
      "max: 1.704154240252739\n",
      "min: -1.3356417810072365\n",
      "avg: 0.5321359532929513\n",
      "best: -1.3356417810072365\n"
     ]
    }
   ],
   "source": [
    "from classes.Population import Population\n",
    "p = Population(4,5)\n",
    "print(f\"population shape: (n_individuals: {len(p.individuals)}, n_elements:{len(p.individuals[0].values)})\")\n",
    "p.evaluate_fitness(eval_fun)\n",
    "print()\n",
    "print(f'all fitnesses: {p.all_fitnesses()}')\n",
    "print(f'max: {p.max_fitness()}')\n",
    "print(f'min: {p.min_fitness()}')\n",
    "print(f'avg: {p.ave_fitness()}')\n",
    "print(f'best: {p.best_fitness(is_min=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Mutation: CustomSigma</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before mutation: [ 0.6158239   0.84011077  0.04183522 -0.75900673  0.6951304  -0.97691133\n",
      "  0.77485725 -0.82851889 -0.95353989 -0.32360095]\n",
      "\n",
      "after mutation: [ 0.96426423  0.03275141 -0.27649359 -2.87685607  1.94059916 -4.15291758\n",
      " -0.72931231 -5.31276875  0.06526971 -0.21640113]\n"
     ]
    }
   ],
   "source": [
    "from classes.Mutation import CustomSigma\n",
    "c = CustomSigma()\n",
    "print(f'before mutation: {i.values}')\n",
    "c.mutate(i)\n",
    "print()\n",
    "print(f'after mutation: {i.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.48956459  0.2666469  -0.65785456 -0.87631033 -0.76161612]\n",
      "[ 0.05075622  0.15102134 -1.52085053 -0.1790282  -0.75827511]\n"
     ]
    }
   ],
   "source": [
    "p = Population(10,5)\n",
    "print(p.individuals[2].values)\n",
    "for ind in p.individuals:\n",
    "    c.mutate(ind)\n",
    "print(p.individuals[2].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> \n",
    "Selection: 1 + l\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent fitnesses: [ 1.52717279 -2.08192512  1.74499241 -2.14277485  2.30525189  1.9969475 ]\n",
      "\n",
      "offspring fitnesses: [ 3.27255515 -0.46249714  1.22047093 -0.34597896 -1.57507415  1.06318626\n",
      "  2.13973693  1.65986068  0.84590833 -1.63149162 -1.6923905   3.43032927\n",
      "  0.04407314  2.97311273  1.59922119 -0.52932659  1.91764334 -1.09584034\n",
      "  1.60882695 -1.36718466 -2.37645832 -1.09046878  0.69811668 -1.69548148\n",
      "  1.40011307 -1.50718636 -2.24696694 -0.92501179  1.73168887  0.07333329]\n",
      "\n",
      "fitnesses after selection: [-2.37645832 -2.24696694 -2.14277485 -2.08192512 -1.69548148 -1.6923905 ]\n"
     ]
    }
   ],
   "source": [
    "parent = Population(6,10)\n",
    "parent.evaluate_fitness(eval_fun)\n",
    "offspring = Population(30,10)\n",
    "offspring.evaluate_fitness(eval_fun)\n",
    "print(f'parent fitnesses: {parent.all_fitnesses()}')\n",
    "print()\n",
    "print(f'offspring fitnesses: {offspring.all_fitnesses()}')\n",
    "print()\n",
    "from classes.Selection import OnePlusL\n",
    "s = OnePlusL()\n",
    "p = s.select(parent, offspring)\n",
    "print(f'fitnesses after selection: {p.all_fitnesses()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> \n",
    "Selection: 1 , l\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parent fitnesses: [-1.26114353  3.13179269  1.21909379  0.4001622  -0.66875803  1.57652244]\n",
      "\n",
      "offspring fitnesses: [ 1.58146625  0.33969361  0.2427774   3.24822923  2.63692412 -1.13540554\n",
      " -2.31387041  1.47698527  2.66437474 -2.60404576  1.48661441 -2.28121047\n",
      "  1.18247853  2.78697945  2.44809518 -0.04351136  1.26451292 -0.22238845\n",
      " -0.14025327 -0.89727187 -1.35612695  0.30318485 -2.49827476  2.34018095\n",
      "  0.79229932  0.03738312  1.45843053 -2.47215704  0.20293497 -0.09998259]\n",
      "\n",
      "fitnesses after selection: [-2.60404576 -2.49827476 -2.47215704 -2.31387041 -2.28121047 -1.35612695]\n"
     ]
    }
   ],
   "source": [
    "parent = Population(6,10)\n",
    "parent.evaluate_fitness(eval_fun)\n",
    "offspring = Population(30,10)\n",
    "offspring.evaluate_fitness(eval_fun)\n",
    "print(f'parent fitnesses: {parent.all_fitnesses()}')\n",
    "print()\n",
    "print(f'offspring fitnesses: {offspring.all_fitnesses()}')\n",
    "print()\n",
    "\n",
    "from classes.Selection import OneCommaL\n",
    "s = OneCommaL()\n",
    "p = s.select(parent, offspring)\n",
    "print(f'fitnesses after selection: {p.all_fitnesses()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Recombination: Intermediate\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<classes.Population.Population object at 0x7f33e00c5b50>\n",
      "[ 0.67195645 -0.65233722  0.12669279 -0.21647548  0.12879082 -0.08978447\n",
      "  0.29330356  0.07824821  0.20135489 -0.29080055]\n",
      "[ 0.91435188 -0.19922402  0.05629444 -0.01934228 -0.57366121 -0.45676124\n",
      " -0.01071739 -0.00893298 -0.09412652  0.58319011]\n",
      "[ 0.50311753 -0.56658393  0.30027835  0.10892739 -0.03211681 -0.09892243\n",
      " -0.40855804  0.05768253  0.23527519 -0.26641827]\n"
     ]
    }
   ],
   "source": [
    "from classes.Recombination import Intermediate\n",
    "parent = Population(6,10)\n",
    "r = Intermediate(3)\n",
    "off = r.recombine(parent)\n",
    "print(off)\n",
    "for i in off.individuals:\n",
    "    print(i.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-18 20:15:57.327465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.330632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.330808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.331172: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-18 20:15:57.332286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.332474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.332695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.598604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.598809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.598978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-18 20:15:57.599137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3294 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to find any matching files for tf_model/model_weights/checkpoint.ckpt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtf_model\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mSimpleClassifier\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m SimpleClassifier()\n",
      "File \u001b[0;32m~/Workdir/EA_ADL/tf_model/SimpleClassifier.py:48\u001b[0m, in \u001b[0;36mSimpleClassifier.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Sequential([\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=31'>32</a>\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_augmentation,\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=32'>33</a>\u001b[0m                     layers\u001b[39m.\u001b[39mRescaling(\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=42'>43</a>\u001b[0m                     layers\u001b[39m.\u001b[39mDense(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=43'>44</a>\u001b[0m                     ])\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=44'>45</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=45'>46</a>\u001b[0m       loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=46'>47</a>\u001b[0m       metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='file:///home/dimi/Workdir/EA_ADL/tf_model/SimpleClassifier.py?line=47'>48</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mload_weights(\u001b[39m'\u001b[39;49m\u001b[39mtf_model/model_weights/checkpoint.ckpt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py:31\u001b[0m, in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=26'>27</a>\u001b[0m error_message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnot found in checkpoint\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=28'>29</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFailed to find any \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=29'>30</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmatching files for\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[0;32m---> <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=30'>31</a>\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mNotFoundError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=31'>32</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSliced checkpoints are not supported\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m error_message \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=32'>33</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mData type \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=33'>34</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=34'>35</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msupported\u001b[39m\u001b[39m'\u001b[39m) \u001b[39min\u001b[39;00m error_message:\n\u001b[1;32m     <a href='file:///home/dimi/.pyenv/versions/3.9.10/envs/tf_env/lib/python3.9/site-packages/tensorflow/python/training/py_checkpoint_reader.py?line=35'>36</a>\u001b[0m   \u001b[39mraise\u001b[39;00m errors_impl\u001b[39m.\u001b[39mUnimplementedError(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, error_message)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for tf_model/model_weights/checkpoint.ckpt"
     ]
    }
   ],
   "source": [
    "from tf_model.SimpleClassifier import *\n",
    "model = SimpleClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1816e5b8539f9acb59161f698974e03141d0f020b6322f72e2cfb5eb3657a0af"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('tf_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
